{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26e07d85",
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy as sp\n",
    "import matplotlib.pyplot as plt\n",
    "import cartopy.crs as ccrs\n",
    "import h5py\n",
    "import os\n",
    "import sys\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import datetime\n",
    "import sklearn\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.tree import export_graphviz\n",
    "from sklearn.inspection import permutation_importance\n",
    "import xarray as xr\n",
    "import statistics as st\n",
    "import random\n",
    "import warnings\n",
    "# Import tensorflow and keras\n",
    "import tensorflow as tf\n",
    "import tensorflow.keras as keras\n",
    "import tensorflow.keras.layers as layers\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from scipy import stats, odr\n",
    "#import shap\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import matplotlib as mpl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f41c18a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "################################################################################\n",
    "# Need to use \"IPWG Extraction Script\" here prior to running any further cells #\n",
    "################################################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d77710b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def my_loss(y_true, y_pred):\n",
    "    # term_1 = tf.math.pow(tf.math.maximum(tf.math.abs(y_true), tf.math.abs(y_pred)), 5.0)  # symmetric\n",
    "    term_1 = tf.math.pow(tf.math.abs(tf.math.maximum(y_true, y_pred)), 10.0)  # asymmetric\n",
    "    term_2 = tf.math.pow((y_true-y_pred), 2.0)\n",
    "    loss_value = tf.multiply(term_1, term_2)\n",
    "    return tf.math.reduce_mean(loss_value)\n",
    "print('Did it work?')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "baf8eb70",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model(x_train, y_train, settings):\n",
    "    # create input layer\n",
    "    input_layer = tf.keras.layers.Input(shape=x_train.shape[1:])\n",
    "\n",
    "    # create a normalization layer if you would like\n",
    "    normalizer = tf.keras.layers.Normalization(axis=(1,))\n",
    "    normalizer.adapt(x_train)\n",
    "    layers = normalizer(input_layer)\n",
    "\n",
    "    # create hidden layers each with specific number of nodes\n",
    "    assert len(settings[\"hiddens\"]) == len(\n",
    "        settings[\"activations\"]\n",
    "    ), \"hiddens and activations settings must be the same length.\"\n",
    "\n",
    "    # add dropout layer\n",
    "    layers = tf.keras.layers.Dropout(rate=settings[\"dropout_rate\"])(layers)\n",
    "\n",
    "    for hidden, activation in zip(settings[\"hiddens\"], settings[\"activations\"]):\n",
    "        layers = tf.keras.layers.Dense(\n",
    "            units=hidden,\n",
    "            activation=activation,\n",
    "            use_bias=True,\n",
    "            kernel_regularizer=tf.keras.regularizers.l1_l2(l1=0, l2=0),\n",
    "            bias_initializer=tf.keras.initializers.RandomNormal(seed=settings[\"random_seed\"]),\n",
    "            kernel_initializer=tf.keras.initializers.RandomNormal(seed=settings[\"random_seed\"]),\n",
    "        )(layers)\n",
    "\n",
    "    # create output layer\n",
    "    output_layer = tf.keras.layers.Dense(\n",
    "        units=1,  # one unit in the output since this is a regression problem\n",
    "        activation=\"linear\",\n",
    "        use_bias=True,\n",
    "        bias_initializer=tf.keras.initializers.RandomNormal(seed=settings[\"random_seed\"] + 1),\n",
    "        kernel_initializer=tf.keras.initializers.RandomNormal(seed=settings[\"random_seed\"] + 2),\n",
    "    )(layers)\n",
    "\n",
    "    # construct the model\n",
    "    model = tf.keras.Model(inputs=input_layer, outputs=output_layer)\n",
    "    model.summary()\n",
    "\n",
    "    return model\n",
    "\n",
    "\n",
    "def compile_model(model, settings):\n",
    "    model.compile(\n",
    "        optimizer=tf.keras.optimizers.Adam(learning_rate=settings[\"learning_rate\"]),\n",
    "        loss=\"mae\",\n",
    "        # loss = my_loss,\n",
    "        metrics=[\n",
    "            \"mse\",\n",
    "            \"mae\"\n",
    "        ],\n",
    "    )\n",
    "    return model\n",
    "print('Did it work?')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdb143e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the data into training and testing sets\n",
    "\n",
    "# Tunable Parameter: Describes the proportion of the dataset we want to use for testing. 1 - split_size is used for training.\n",
    "split_size = 0.2\n",
    "\n",
    "# PARAMETERS:\n",
    "#     test_size: fraction of testing/validation datasets\n",
    "#     random_state: random parameter\n",
    "Xtrain, Xval, Ytrain, Yval = train_test_split(\n",
    "    input_raw, output_raw, test_size=split_size, random_state=42\n",
    ")\n",
    "print('Did it work?')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7585a645",
   "metadata": {},
   "outputs": [],
   "source": [
    "settings = {\n",
    "    \"hiddens\": [3, 3],\n",
    "    \"activations\": [\"relu\", \"relu\"],\n",
    "    \"learning_rate\": 0.0025,\n",
    "    \"random_seed\": 33,\n",
    "    \"max_epochs\": 5_000,\n",
    "    \"batch_size\": 256,\n",
    "    \"patience\": 10,\n",
    "    \"dropout_rate\": 0.,\n",
    "}\n",
    "\n",
    "tf.keras.backend.clear_session()\n",
    "tf.keras.utils.set_random_seed(settings[\"random_seed\"])\n",
    "\n",
    "model = build_model(Xtrain, Ytrain, settings)\n",
    "model = compile_model(model, settings)\n",
    "print('Did it work?')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0361327",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the early stopping callback\n",
    "early_stopping_callback = tf.keras.callbacks.EarlyStopping(\n",
    "    monitor=\"val_loss\", patience=settings[\"patience\"], restore_best_weights=True, mode=\"auto\", verbose=1,\n",
    ")\n",
    "\n",
    "# train the model via model.fit\n",
    "history = model.fit(\n",
    "    Xtrain,\n",
    "    Ytrain,\n",
    "    epochs=settings[\"max_epochs\"],\n",
    "    batch_size=settings[\"batch_size\"],\n",
    "    shuffle=True,\n",
    "    validation_data=[Xval, Yval],\n",
    "    callbacks=[early_stopping_callback],\n",
    "    verbose=2,\n",
    ")\n",
    "print('Did it work?')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ae78231",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's plot the change in loss and categorical_accuracy\n",
    "\n",
    "fig, axs = plt.subplots(1, 2, figsize=(10, 4))\n",
    "\n",
    "axs[0].plot(history.history[\"loss\"], label=\"training\")\n",
    "axs[0].plot(history.history[\"val_loss\"], label=\"validation\")\n",
    "axs[0].set_xlabel(\"Epoch\")\n",
    "axs[0].set_ylabel(\"Loss\")\n",
    "axs[0].legend()\n",
    "\n",
    "axs[1].plot(history.history[\"mae\"], label=\"training\")\n",
    "axs[1].plot(history.history[\"val_mae\"], label=\"validation\")\n",
    "axs[1].set_xlabel(\"Epoch\")\n",
    "axs[1].set_ylabel(\"Mean Absolute Error\")\n",
    "axs[1].legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47e200c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "###########################################################\n",
    "#Training same model as above but with the criteria as MSE#\n",
    "###########################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad606ee8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def my_loss(y_true, y_pred):\n",
    "    # term_1 = tf.math.pow(tf.math.maximum(tf.math.abs(y_true), tf.math.abs(y_pred)), 5.0)  # symmetric\n",
    "    term_1 = tf.math.pow(tf.math.abs(tf.math.maximum(y_true, y_pred)), 10.0)  # asymmetric\n",
    "    term_2 = tf.math.pow((y_true-y_pred), 2.0)\n",
    "    loss_value = tf.multiply(term_1, term_2)\n",
    "    return tf.math.reduce_mean(loss_value)\n",
    "print('Did it work?')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4361ab07",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model(x_train, y_train, settings):\n",
    "    # create input layer\n",
    "    input_layer = tf.keras.layers.Input(shape=x_train.shape[1:])\n",
    "\n",
    "    # create a normalization layer if you would like\n",
    "    normalizer = tf.keras.layers.Normalization(axis=(1,))\n",
    "    normalizer.adapt(x_train)\n",
    "    layers = normalizer(input_layer)\n",
    "\n",
    "    # create hidden layers each with specific number of nodes\n",
    "    assert len(settings[\"hiddens\"]) == len(\n",
    "        settings[\"activations\"]\n",
    "    ), \"hiddens and activations settings must be the same length.\"\n",
    "\n",
    "    # add dropout layer\n",
    "    layers = tf.keras.layers.Dropout(rate=settings[\"dropout_rate\"])(layers)\n",
    "\n",
    "    for hidden, activation in zip(settings[\"hiddens\"], settings[\"activations\"]):\n",
    "        layers = tf.keras.layers.Dense(\n",
    "            units=hidden,\n",
    "            activation=activation,\n",
    "            use_bias=True,\n",
    "            kernel_regularizer=tf.keras.regularizers.l1_l2(l1=0, l2=0),\n",
    "            bias_initializer=tf.keras.initializers.RandomNormal(seed=settings[\"random_seed\"]),\n",
    "            kernel_initializer=tf.keras.initializers.RandomNormal(seed=settings[\"random_seed\"]),\n",
    "        )(layers)\n",
    "\n",
    "    # create output layer\n",
    "    output_layer = tf.keras.layers.Dense(\n",
    "        units=1,  # one unit in the output since this is a regression problem\n",
    "        activation=\"linear\",\n",
    "        use_bias=True,\n",
    "        bias_initializer=tf.keras.initializers.RandomNormal(seed=settings[\"random_seed\"] + 1),\n",
    "        kernel_initializer=tf.keras.initializers.RandomNormal(seed=settings[\"random_seed\"] + 2),\n",
    "    )(layers)\n",
    "\n",
    "    # construct the model\n",
    "    model = tf.keras.Model(inputs=input_layer, outputs=output_layer)\n",
    "    model.summary()\n",
    "\n",
    "    return model\n",
    "\n",
    "\n",
    "def compile_model(model, settings):\n",
    "    model.compile(\n",
    "        optimizer=tf.keras.optimizers.Adam(learning_rate=settings[\"learning_rate\"]),\n",
    "        loss=\"mse\",\n",
    "        # loss = my_loss,\n",
    "        metrics=[\n",
    "            \"mse\",\n",
    "            \"mae\"\n",
    "        ],\n",
    "    )\n",
    "    return model\n",
    "print('Did it work?')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "296a8235",
   "metadata": {},
   "outputs": [],
   "source": [
    "settings = {\n",
    "    \"hiddens\": [3, 3],\n",
    "    \"activations\": [\"relu\", \"relu\"],\n",
    "    \"learning_rate\": 0.0025,\n",
    "    \"random_seed\": 33,\n",
    "    \"max_epochs\": 5_000,\n",
    "    \"batch_size\": 256,\n",
    "    \"patience\": 10,\n",
    "    \"dropout_rate\": 0.,\n",
    "}\n",
    "\n",
    "tf.keras.backend.clear_session()\n",
    "tf.keras.utils.set_random_seed(settings[\"random_seed\"])\n",
    "\n",
    "model = build_model(Xtrain, Ytrain, settings)\n",
    "model = compile_model(model, settings)\n",
    "print('Did it work?')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9236432",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the early stopping callback\n",
    "early_stopping_callback = tf.keras.callbacks.EarlyStopping(\n",
    "    monitor=\"val_loss\", patience=settings[\"patience\"], restore_best_weights=True, mode=\"auto\", verbose=1,\n",
    ")\n",
    "\n",
    "# train the model via model.fit\n",
    "history = model.fit(\n",
    "    Xtrain,\n",
    "    Ytrain,\n",
    "    epochs=settings[\"max_epochs\"],\n",
    "    batch_size=settings[\"batch_size\"],\n",
    "    shuffle=True,\n",
    "    validation_data=[Xval, Yval],\n",
    "    callbacks=[early_stopping_callback],\n",
    "    verbose=2,\n",
    ")\n",
    "print('Did it work?')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46fa0721",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's plot the change in loss and categorical_accuracy\n",
    "\n",
    "fig, axs = plt.subplots(1, 2, figsize=(10, 4))\n",
    "\n",
    "axs[0].plot(history.history[\"loss\"], label=\"training\")\n",
    "axs[0].plot(history.history[\"val_loss\"], label=\"validation\")\n",
    "axs[0].set_xlabel(\"Epoch\")\n",
    "axs[0].set_ylabel(\"Loss\")\n",
    "axs[0].legend()\n",
    "\n",
    "axs[1].plot(history.history[\"mae\"], label=\"training\")\n",
    "axs[1].plot(history.history[\"val_mae\"], label=\"validation\")\n",
    "axs[1].set_xlabel(\"Epoch\")\n",
    "axs[1].set_ylabel(\"Mean Absolute Error\")\n",
    "axs[1].legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d10e390",
   "metadata": {},
   "outputs": [],
   "source": [
    "#######################################################\n",
    "# Pick the better model or do the following with both #\n",
    "#######################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e25f2b63",
   "metadata": {},
   "outputs": [],
   "source": [
    "########################\n",
    "# Read in Testing Data #\n",
    "########################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdc63001",
   "metadata": {},
   "outputs": [],
   "source": [
    "# What predictions did the model make for our training, validation, and test sets?\n",
    "Ptrain = model.predict(Xtrain)  # Array of predictions\n",
    "Pval = model.predict(Xval)\n",
    "Ptest = model.predict(Xtest)\n",
    "print('Did it work?')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a20c91ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "error_test = np.abs(Pval - Yval)[:,0]\n",
    "plt.figure()\n",
    "plt.plot(Yval, error_test, \".\")\n",
    "plt.xlabel(\"true value\")\n",
    "#plt.ylabel(\"predicted value\")\n",
    "plt.ylabel(\"error_test\")\n",
    "plt.show()\n",
    "#features = np.array(data_tbu[INPUT_VARIABLES])\n",
    "#feature_list = list(features.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af454e66",
   "metadata": {},
   "outputs": [],
   "source": [
    "error_test_t = np.abs(Ptest - Ytest)[:,0]\n",
    "plt.figure()\n",
    "plt.plot(Ytest, error_test_t, \".\")\n",
    "plt.xlabel(\"true value\")\n",
    "#plt.ylabel(\"predicted value\")\n",
    "plt.ylabel(\"error_test\")\n",
    "plt.show()\n",
    "#features = np.array(data_tbu[INPUT_VARIABLES])\n",
    "#feature_list = list(features.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "705fff3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(Yval, Yval, 'bo', label = 'tbs_atms')\n",
    "plt.plot(Yval, Pval, 'rx', label = 'tbs_atms')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60cbf4c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(Ytest, Ytest, 'bo', label = 'tbs_atms')\n",
    "plt.plot(Ytest, Ptest, 'rx', label = 'tbs_atms')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0781f17",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "\n",
    "MSEtr = mean_squared_error(Ytrain,Ptrain)\n",
    "MAEtr = mean_absolute_error(Ytrain,Ptrain)\n",
    "MSEv = mean_squared_error(Yval,Pval)\n",
    "MAEv = mean_absolute_error(Yval,Pval)\n",
    "MSEt = mean_squared_error(Ytest,Ptest)\n",
    "MAEt = mean_absolute_error(Ytest,Ptest)\n",
    "\n",
    "#print('Training MSE : ', MSEtr)\n",
    "#print('Training MAE : ', MAEtr)\n",
    "print('-----------------------------------')\n",
    "print('Validation MSE : ', MSEv)\n",
    "print('Validation MAE : ', MAEv)\n",
    "print('-----------------------------------')\n",
    "print('Testing MSE : ', MSEt)\n",
    "print('Testing MAE : ', MAEt)\n",
    "print('-----------------------------------')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8b2d649",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
